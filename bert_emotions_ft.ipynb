{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import wandb"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# choose labels subset\n","ekman_emotions = False\n","FSJ_emotions = True\n","FSJN_emotions = False\n","all_emotions = False\n","full_ekman_emotions = False\n","sentiment = False\n","sentiment_3 = False"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["all_emotions_list = ['admiration', 'amusement',\n","       'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n","       'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n","       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n","       'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n","       'sadness', 'surprise', 'neutral']"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14166,"status":"ok","timestamp":1650427720207,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"AXhepfO2SumU"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/svetlana.maslenkova/.conda/envs/nlp_project_env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pickle5 as pkl\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import transformers\n","from transformers import AutoModel, BertTokenizerFast\n","\n","# specify GPU\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1650427720209,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"SOaFpw_hy5BA","outputId":"dbeaa9a9-36ca-4ed8-befa-256f819d304c"},"outputs":[{"data":{"text/plain":["'/home/svetlana.maslenkova/Documents/nlp_project'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["import os, sys\n","\n","cwd = os.getcwd()\n","cwd"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":920,"status":"ok","timestamp":1650427758997,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"uqIm07DBob1c"},"outputs":[],"source":["EMO_DATA_PATH = cwd + '/data/go_emotions_dataset.csv'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":829,"status":"ok","timestamp":1650389785348,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"L9gD8WlQo5C_","outputId":"1f1a280d-1329-463e-b74e-dd84e53916cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["(57730, 31)\n","Index(['id', 'text', 'example_very_unclear', 'admiration', 'amusement',\n","       'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n","       'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n","       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n","       'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n","       'sadness', 'surprise', 'neutral'],\n","      dtype='object')\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>example_very_unclear</th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>...</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>eew5j0j</td>\n","      <td>That game hurt.</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ed2mah1</td>\n","      <td>You do right, if you don't care then fuck 'em!</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>eeibobj</td>\n","      <td>Man I love reddit.</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>eda6yn6</td>\n","      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>eespn2i</td>\n","      <td>Right? Considering it’s such an important docu...</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>211088</th>\n","      <td>ef0lnq5</td>\n","      <td>I didn’t even notice that one. I noticed the b...</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>211094</th>\n","      <td>edirq0m</td>\n","      <td>Thanks, [NAME]</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211095</th>\n","      <td>ee6pagw</td>\n","      <td>Everyone likes [NAME].</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>211138</th>\n","      <td>eegg1aj</td>\n","      <td>Projecting pretty hard here.</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>211161</th>\n","      <td>ee7fqma</td>\n","      <td>I just called the Capitol Police. They are not...</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>57730 rows × 31 columns</p>\n","</div>"],"text/plain":["             id                                               text  \\\n","0       eew5j0j                                    That game hurt.   \n","2       ed2mah1     You do right, if you don't care then fuck 'em!   \n","3       eeibobj                                 Man I love reddit.   \n","4       eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n","5       eespn2i  Right? Considering it’s such an important docu...   \n","...         ...                                                ...   \n","211088  ef0lnq5  I didn’t even notice that one. I noticed the b...   \n","211094  edirq0m                                     Thanks, [NAME]   \n","211095  ee6pagw                             Everyone likes [NAME].   \n","211138  eegg1aj                       Projecting pretty hard here.   \n","211161  ee7fqma  I just called the Capitol Police. They are not...   \n","\n","        example_very_unclear  admiration  amusement  anger  annoyance  \\\n","0                      False           0          0      0          0   \n","2                      False           0          0      0          0   \n","3                      False           0          0      0          0   \n","4                      False           0          0      0          0   \n","5                      False           0          0      0          0   \n","...                      ...         ...        ...    ...        ...   \n","211088                 False           0          0      0          0   \n","211094                 False           0          0      0          0   \n","211095                 False           0          0      0          0   \n","211138                 False           0          0      0          0   \n","211161                 False           0          0      0          0   \n","\n","        approval  caring  confusion  ...  love  nervousness  optimism  pride  \\\n","0              0       0          0  ...     0            0         0      0   \n","2              0       0          0  ...     0            0         0      0   \n","3              0       0          0  ...     1            0         0      0   \n","4              0       0          0  ...     0            0         0      0   \n","5              0       0          0  ...     0            0         0      0   \n","...          ...     ...        ...  ...   ...          ...       ...    ...   \n","211088         0       0          0  ...     0            0         0      0   \n","211094         0       0          0  ...     0            0         0      0   \n","211095         0       0          0  ...     1            0         0      0   \n","211138         0       0          0  ...     0            0         0      0   \n","211161         0       0          0  ...     0            0         0      0   \n","\n","        realization  relief  remorse  sadness  surprise  neutral  \n","0                 0       0        0        1         0        0  \n","2                 0       0        0        0         0        1  \n","3                 0       0        0        0         0        0  \n","4                 0       0        0        0         0        1  \n","5                 0       0        0        0         0        0  \n","...             ...     ...      ...      ...       ...      ...  \n","211088            0       0        0        0         0        1  \n","211094            0       0        0        0         0        0  \n","211095            0       0        0        0         0        0  \n","211138            0       0        0        0         0        1  \n","211161            0       1        0        0         0        0  \n","\n","[57730 rows x 31 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["row_data = pd.read_csv(EMO_DATA_PATH)\n","# remove unclear samples\n","row_data = row_data[row_data.example_very_unclear==False]\n","# remove duplicated samples\n","row_data = row_data.drop_duplicates(subset='text', keep='first')\n","\n","print(row_data.shape)\n","print(row_data.columns)\n","row_data"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":475,"status":"ok","timestamp":1650389788110,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"Q8NNCTZ_3ohw","outputId":"ebc64327-04ac-47db-e36a-710a522211f3"},"outputs":[{"data":{"text/plain":["neutral           15803\n","approval           4898\n","admiration         4874\n","annoyance          3534\n","gratitude          3468\n","disapproval        3106\n","curiosity          2808\n","amusement          2684\n","optimism           2448\n","realization        2385\n","love               2370\n","anger              2242\n","joy                2205\n","disappointment     2193\n","confusion          2023\n","sadness            1851\n","caring             1611\n","excitement         1558\n","surprise           1528\n","disgust            1368\n","desire             1014\n","fear                891\n","remorse             714\n","embarrassment       637\n","nervousness         489\n","relief              372\n","pride               354\n","grief               180\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["emo_count = row_data[['admiration', 'amusement',\n","       'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n","       'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n","       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n","       'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n","       'sadness', 'surprise', 'neutral']].sum().sort_values(ascending=False)\n","\n","emo_count"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"imYjMrEI4ENQ"},"outputs":[],"source":["if sentiment:\n","    neg_emo = [ 'annoyance', 'disappointment', 'sadness', 'disapproval', 'anger', 'sadness', 'disgust', 'fear', 'embarrassment', 'grief']\n","    neu_emo = ['neutral']\n","    pos_emo = ['approval', 'admiration', 'gratitude', 'curiosity', 'optimism', 'love', 'joy', 'caring', 'excitement', 'surprise', 'pride', 'relief', ]\n","    mix_emo = ['amusement', 'realization', 'confusion',  'desire', 'remorse', 'nervousness']\n","    \n","if full_ekman_emotions:\n","    anger_emo = [\"anger\", \"annoyance\", \"disapproval\"]\n","    disgust_emo = [\"disgust\"]\n","    fear_emo = [\"fear\", \"nervousness\"]\n","    joy_emo =  [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"]\n","    sadness_emo = [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"]\n","    surprise_emo = [\"surprise\", \"realization\", \"confusion\", \"curiosity\"]\n","    neutral_emo = ['neutral']\n","\n","if sentiment_3:\n","    positive = [\"amusement\", \"excitement\", \"joy\", \"love\", \"desire\", \"optimism\", \"caring\", \"pride\", \"admiration\", \"gratitude\", \"relief\", \"approval\"]\n","    negative = [\"fear\", \"nervousness\", \"remorse\", \"embarrassment\", \"disappointment\", \"sadness\", \"grief\", \"disgust\", \"anger\", \"annoyance\", \"disapproval\"]\n","    ambiguous = [\"realization\", \"surprise\", \"curiosity\", \"confusion\"]"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# emo_df = row_data[['text'] + all_emotions_list].copy()\n","if sentiment_3:\n","    emo_df = row_data[['text'] + positive +negative+ambiguous].copy()\n","\n","if ekman_emotions:\n","    emo_df = row_data[['text']+['joy',  'fear', 'anger',  'surprise', 'disgust',  'sadness']].copy()\n","\n","if FSJ_emotions:\n","    emo_df = row_data[['text']+['fear', 'sadness', 'joy']].copy()\n","    emo_df = emo_df[emo_df[['fear', 'sadness', 'joy']].sum(axis=1)>0]\n","\n","if FSJN_emotions:\n","    emo_df = row_data[['text']+['fear', 'sadness', 'joy', 'neutral']].copy()\n","    emo_df = emo_df[emo_df[['fear', 'sadness', 'joy', 'neutral']].sum(axis=1)>0]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qIJYnF7o-Gpc"},"outputs":[],"source":["if sentiment:\n","    S = row_data[neg_emo].copy()\n","    neg_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[neu_emo].copy()\n","    neu_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[pos_emo].copy()\n","    pos_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[mix_emo].copy()\n","    mix_ids = S[S.sum(axis=1)>0].index.to_list()\n","\n","if full_ekman_emotions:\n","    S = row_data[anger_emo].copy()\n","    anger_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[disgust_emo].copy()\n","    disguist_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[fear_emo].copy()\n","    fear_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[joy_emo].copy()\n","    joy_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[sadness_emo].copy()\n","    sadness_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[surprise_emo].copy()\n","    surprise_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[neutral_emo].copy()\n","    neutral_ids = S[S.sum(axis=1)>0].index.to_list()\n","\n","if sentiment:\n","    S = row_data[positive].copy()\n","    positive_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[negative].copy()\n","    negative_ids = S[S.sum(axis=1)>0].index.to_list()\n","    S = row_data[ambiguous].copy()\n","    ambiguous_ids = S[S.sum(axis=1)>0].index.to_list()\n","\n","if FSJN_emotions:\n","    neu_ids = emo_df[emo_df.neutral==1].sample(frac=0.1).index.to_list()\n","    fear_ids = emo_df[emo_df.fear==1].index.to_list()\n","    sadness_ids = emo_df[emo_df.sadness==1].index.to_list()\n","    joy_ids = emo_df[emo_df.joy==1].index.to_list()\n","    emo_df = emo_df[emo_df.index.isin(neu_ids+fear_ids+sadness_ids+joy_ids)].copy().sample(frac=1)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GL-ZzzRPAJxZ"},"outputs":[],"source":["if four_cat_emotions:\n","    pos = [1 if id in pos_ids else 0 for id in emo_df.index]\n","    neg = [1 if id in neg_ids else 0 for id in emo_df.index]\n","    neu = [1 if id in neu_ids else 0 for id in emo_df.index]\n","    mix = [1 if id in mix_ids else 0 for id in emo_df.index]\n","\n","if full_ekman_emotions:\n","    anger = [1 if id in anger_ids else 0 for id in emo_df.index]\n","    disguist = [1 if id in disguist_ids else 0 for id in emo_df.index]\n","    fear = [1 if id in fear_ids else 0 for id in emo_df.index]\n","    joy = [1 if id in joy_ids else 0 for id in emo_df.index]\n","    sadness = [1 if id in sadness_ids else 0 for id in emo_df.index]\n","    surprise = [1 if id in surprise_ids else 0 for id in emo_df.index]\n","    neutral = [1 if id in neutral_ids else 0 for id in emo_df.index]\n","\n","if sentiment:\n","    pos = [1 if id in positive_ids else 0 for id in emo_df.index]\n","    neg = [1 if id in negative_ids else 0 for id in emo_df.index]\n","    ambig = [1 if id in ambiguous_ids else 0 for id in emo_df.index]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":545,"status":"ok","timestamp":1650392520538,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"DnUE9RKI_OMs","outputId":"6d7b6ee4-3e4e-40cb-b78a-1009f35e41ae"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>fear</th>\n","      <th>sadness</th>\n","      <th>joy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>That game hurt.</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>So happy for [NAME]. So sad he's not here. Ima...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>By far the coolest thing I've seen on this thr...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>Sending love and strength vibes &lt;3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>Me too! First time in a couple of years!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 text  fear  sadness  joy\n","0                                     That game hurt.     0        1    0\n","29  So happy for [NAME]. So sad he's not here. Ima...     0        1    1\n","43  By far the coolest thing I've seen on this thr...     0        0    1\n","49                 Sending love and strength vibes <3     0        0    1\n","58           Me too! First time in a couple of years!     0        0    1"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["if sentiment:\n","    emo_df = row_data[['text']].copy()\n","    emo_df['positive'] = pos\n","    emo_df['negative'] = neg\n","    emo_df['neutral'] = neu\n","    emo_df['mixed'] = mix\n","\n","if full_ekman_emotions:\n","    emo_df = row_data[['text']].copy()\n","    emo_df['anger'] = anger\n","    emo_df['disguist'] = disguist\n","    emo_df['joy'] = joy\n","    emo_df['sadness'] = sadness\n","    emo_df['surprise'] = surprise\n","    emo_df['neutral'] = neutral\n","\n","if sentiment_3:\n","    emo_df = row_data[['text']].copy()\n","    emo_df['positive'] = pos\n","    emo_df['negative'] = neg\n","    emo_df['ambiguous'] = ambig\n","\n","emo_df.head()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1650379456037,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"Y81UrtFSp2Vk","outputId":"ea910083-27fe-44f4-a962-fa6d780b20f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["(35400, 7)\n"]}],"source":["if ekman_emotions:\n","    emo_df = row_data[['text', 'joy', 'fear', 'anger', 'surprise', 'disgust', 'sadness']].copy()\n","    emo_df = emo_df[emo_df[['joy', 'fear', 'anger', 'surprise', 'disgust', 'sadness']].sum(axis=1)>0]\n","\n","if all_emotions:    \n","    emo_df = row_data[['text', 'admiration', 'amusement',\n","        'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n","        'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n","        'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n","        'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n","        'sadness', 'surprise', 'neutral']].copy()\n","       \n","print(emo_df.shape)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"K5y526xYof8y"},"outputs":[],"source":["emo_df['WORD_COUNT'] = emo_df['text'].apply(lambda x: len(x.split()))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":509,"status":"ok","timestamp":1650396703157,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"FQi-C49rriCL"},"outputs":[],"source":["if FSJ_emotions:\n","    emo_df['target_list'] = list(zip(emo_df.fear, emo_df.sadness, emo_df.joy))\n","if FSJN_emotions:\n","    emo_df['target_list'] = list(zip(emo_df.fear, emo_df.sadness, emo_df.joy, emo_df.neutral))\n","if ekman_emotions:\n","    emo_df['target_list'] = list(zip(emo_df.joy, emo_df.fear, emo_df.anger, emo_df.surprise, emo_df.disgust, emo_df.sadness))\n","if sentiment:\n","    emo_df['target_list'] = list(zip(emo_df.positive, emo_df.negative, emo_df.neutral, emo_df.mixed))\n","if all_emotions:    \n","    emo_df['target_list'] = list(zip(emo_df.admiration, emo_df.amusement, emo_df.anger, emo_df.annoyance, \\\n","                            emo_df.approval, emo_df.caring, emo_df.confusion, emo_df.curiosity, \\\n","                                emo_df.desire, emo_df.disappointment, emo_df.disapproval, emo_df.disgust,\\\n","                                     emo_df.embarrassment, emo_df.excitement, emo_df.fear, emo_df.gratitude, \\\n","                                         emo_df.grief, emo_df.joy, emo_df.love, emo_df.nervousness, emo_df.optimism, \\\n","                                             emo_df.pride, emo_df.realization, emo_df.relief, emo_df.remorse,\\\n","                                                 emo_df.sadness, emo_df.surprise, emo_df.neutral))\n","\n","if full_ekman_emotions:\n","    emo_df['target_list'] = list(zip(emo_df.anger, emo_df.disguist, emo_df.joy, emo_df.sadness, emo_df.surprise, emo_df.neutral))\n","\n","if sentiment_3:\n","    emo_df['target_list'] = list(zip(emo_df.positive, emo_df.negative, emo_df.ambiguous))\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":526,"status":"ok","timestamp":1650396705933,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"pr8zowWjsjTJ","outputId":"01b4ecf2-73d9-4866-b416-3e9c9a4a2b6d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>fear</th>\n","      <th>sadness</th>\n","      <th>joy</th>\n","      <th>target_list</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>That game hurt.</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>(0, 1, 0)</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>So happy for [NAME]. So sad he's not here. Ima...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>(0, 1, 1)</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 text  fear  sadness  joy  \\\n","0                                     That game hurt.     0        1    0   \n","29  So happy for [NAME]. So sad he's not here. Ima...     0        1    1   \n","\n","   target_list  \n","0    (0, 1, 0)  \n","29   (0, 1, 1)  "]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["emo_df.head(2)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["(35400, 9)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["emo_df.shape"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1650396710472,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"8x2xJ3DTBO92","outputId":"c1c61c8d-e8fc-4c5c-f406-795e5dbae1d4"},"outputs":[{"data":{"text/plain":["[4.53, 1.61, 1.21]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# calculate weights for the loss\n","\n","pos_weights = []\n","\n","if FSJ_emotions:\n","    for emotion in ['fear',\t'sadness',\t'joy']:\n","        pos_weights.append(np.round(np.sum(emo_df[emotion]==0) / np.sum(emo_df[emotion]==1),2))   \n","\n","if FSJN_emotions:\n","    for emotion in ['fear',\t'sadness',\t'joy', 'neutral']:\n","        pos_weights.append(np.round(np.sum(emo_df[emotion]==0) / np.sum(emo_df[emotion]==1),2))   \n","        \n","if ekman_emotions:\n","    pos_weights.append(np.round(np.sum(emo_df.joy==0) / np.sum(emo_df.joy==1),2))\n","    pos_weights.append(np.round(np.sum(emo_df.fear==0) / np.sum(emo_df.fear==1)))\n","    pos_weights.append(np.round(np.sum(emo_df.anger==0) / np.sum(emo_df.anger==1)))\n","    pos_weights.append(np.round(np.sum(emo_df.surprise==0) / np.sum(emo_df.surprise==1)))\n","    pos_weights.append(np.round(np.sum(emo_df.disgust==0) / np.sum(emo_df.disgust==1)))\n","    pos_weights.append(np.round(np.sum(emo_df.sadness==0) / np.sum(emo_df.sadness==1)))\n","\n","if sentiment:\n","    pos_weights.append(np.round(np.sum(emo_df.positive==0) / np.sum(emo_df.positive==1),2))\n","    pos_weights.append(np.round(np.sum(emo_df.negative==0) / np.sum(emo_df.negative==1),2))\n","    pos_weights.append(np.round(np.sum(emo_df.neutral==0) / np.sum(emo_df.neutral==1),2))\n","    pos_weights.append(np.round(np.sum(emo_df.mixed==0) / np.sum(emo_df.mixed==1),2))\n","\n","if all_emotions:\n","    for emotion in all_emotions_list:\n","        pos_weights.append(np.round(np.sum(emo_df[emotion]==0) / np.sum(emo_df[emotion]==1),2))\n","    \n","if full_ekman_emotions:\n","    for emotion in ['anger',\t'disguist',\t'joy',\t'sadness',\t'surprise',\t'neutral']:\n","        pos_weights.append(np.round(np.sum(emo_df[emotion]==0) / np.sum(emo_df[emotion]==1),2))\n","\n","if sentiment_3:\n","    for emotion in ['positive',\t'negative',\t'ambiguous']:\n","        pos_weights.append(np.round(np.sum(emo_df[emotion]==0) / np.sum(emo_df[emotion]==1),2))   \n","\n","\n","pos_weights"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4291,"status":"ok","timestamp":1650397382376,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"UDAseYH_pKcq"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Defining some key variables that will be used later on in the training\n","MAX_LEN = 16\n","BATCH_SIZE = 900\n","EPOCHS = 30\n","LEARNING_RATE = 1e-05\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1650396739722,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"s2N2sGNmpSip"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe['text']\n","        self.targets = self.data.target_list\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index]).lower()\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1650397382377,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"Pjy3r8_NvE_9","outputId":"69a36a24-8f70-4fcb-93fb-1f2bb71148eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["FULL Dataset: (17669, 5)\n","TRAIN Dataset: (15902, 5)\n","VALID Dataset: (883, 5)\n","TEST Dataset: (884, 5)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","train_dataset, temp_dataset = train_test_split(emo_df, random_state=43, test_size = 0.1)\n","temp_dataset = temp_dataset.reset_index(drop=True)\n","valid_dataset, test_dataset = train_test_split(temp_dataset, random_state=43, test_size = 0.5)\n","valid_dataset = valid_dataset.reset_index(drop=True)\n","test_dataset = test_dataset.reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","\n","print(\"FULL Dataset: {}\".format(emo_df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"VALID Dataset: {}\".format(valid_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n","validation_set = CustomDataset(valid_dataset, tokenizer, MAX_LEN)\n","test_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n","\n","train_loader = DataLoader(training_set, batch_size=BATCH_SIZE , shuffle=True)\n","valid_loader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3198,"status":"ok","timestamp":1650397308509,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"7dR2qv2TveI8","outputId":"681b4e35-8242-41a0-92d1-d0eadde6316a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n","        self.l2 = torch.nn.Dropout(0.3)\n","        self.lstm = nn.LSTM(input_size=768,\n","                            hidden_size=128,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.l3 = torch.nn.Linear(256, 3)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        _, output= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n","        output = self.l2(output).unsqueeze(1)\n","        output, _ = self.lstm(output)\n","        output = self.l3(output).squeeze(1)\n","        return output\n","\n","model = BERTClass()"]},{"cell_type":"markdown","metadata":{"id":"heU7775eyEmv"},"source":["### Functions"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650396780587,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"a56WJJ9UxTB4"},"outputs":[],"source":["def load_ckp(checkpoint_fpath, model, optimizer, gpu=True):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    if gpu==False:\n","      map_location=torch.device('cpu')\n","    else:\n","      map_location = torch.device('cuda')\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath, map_location=map_location)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min#.item()"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650396780588,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"YfhkvGkjyKHS"},"outputs":[],"source":["import shutil, sys   \n","def save_ckp(state, is_best, checkpoint_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    # if is_best:\n","    #     best_fpath = best_model_path\n","    #     # copy that checkpoint file to best path given, best_model_path\n","    #     shutil.copyfile(f_path, best_fpath)"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1650396780589,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"4RaPfyhNyOve"},"outputs":[],"source":["def train_model(start_epochs,  n_epochs, valid_loss_min_input, \n","                training_loader, validation_loader, model, \n","                optimizer, checkpoint_path):\n","   \n","  # initialize tracker for minimum validation loss\n","  valid_loss_min = valid_loss_min_input \n","   \n"," \n","  for epoch in range(start_epochs, n_epochs+1):\n","    running_train_loss = 0.0\n","    running_valid_loss = 0\n","    global_step = 0\n","    epoch_patience = 0\n","    valid_targets = []\n","\n","    model.train()\n","    print('############# Epoch {}: Training Start   #############'.format(epoch))\n","    for batch_idx, data in enumerate(training_loader):\n","        #print('yyy epoch', batch_idx)\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(torch.sigmoid(outputs), targets)\n","        #if batch_idx%5000==0:\n","         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #print('before loss data in training', loss.item(), train_loss)\n","        running_train_loss += loss.item() * targets.size(0)\n","        global_step += 1\n","        #print('after loss data in training', loss.item(), train_loss)\n","        wandb.log({'train_loss':loss.item(), 'global_step':global_step})\n","    \n","    print('############# Epoch {}: Training End     #############'.format(epoch))\n","    \n","    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","    ######################    \n","    # validate the model #\n","    ######################\n"," \n","    model.eval()\n","   \n","    with torch.no_grad():\n","      for batch_idx, data in enumerate(validation_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            outputs = model(ids, mask, token_type_ids)\n","\n","            loss = loss_fn(torch.sigmoid(outputs), targets)\n","            running_valid_loss += loss.item() * targets.size(0)\n","            # valid_targets.extend(targets.cpu().detach().numpy().tolist())\n","            # val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","      print('############# Epoch {}: Validation End     #############'.format(epoch))\n","      # calculate average losses\n","      #print('before cal avg train loss', train_loss)\n","      train_loss = running_train_loss/len(training_loader.dataset)\n","      valid_loss = running_valid_loss/len(validation_loader.dataset)\n","      # print training/validation statistics \n","      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n","            epoch, \n","            train_loss,\n","            valid_loss\n","            ))\n","      wandb.log({'epoch_train_loss':train_loss, 'epoch_val_loss':valid_loss, 'epoch':epoch})\n","      # create checkpoint variable and add important data\n","      checkpoint = {\n","            'epoch': epoch + 1,\n","            'valid_loss_min': valid_loss,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict()\n","      }\n","        \n","        # save checkpoint\n","      # save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","        \n","      ## TODO: save the model if validation loss has decreased\n","      if valid_loss < valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n","        # save checkpoint as best model\n","        save_ckp(checkpoint, True, checkpoint_path)\n","        valid_loss_min = valid_loss\n","        epoch_patience = 0\n","      else:\n","        epoch_patience += 1\n","\n","    if epoch_patience >= 3:\n","      break\n","\n","    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","\n","\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"mfRHcaQx6VGl"},"source":["### Training"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["model = BERTClass()\n","model = model.to(device)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\t l1.embeddings.word_embeddings.weight True\n","\t l1.embeddings.position_embeddings.weight True\n","\t l1.embeddings.token_type_embeddings.weight True\n","\t l1.embeddings.LayerNorm.weight True\n","\t l1.embeddings.LayerNorm.bias True\n","\t l1.encoder.layer.0.attention.self.query.weight False\n","\t l1.encoder.layer.0.attention.self.query.bias False\n","\t l1.encoder.layer.0.attention.self.key.weight False\n","\t l1.encoder.layer.0.attention.self.key.bias False\n","\t l1.encoder.layer.0.attention.self.value.weight False\n","\t l1.encoder.layer.0.attention.self.value.bias False\n","\t l1.encoder.layer.0.attention.output.dense.weight False\n","\t l1.encoder.layer.0.attention.output.dense.bias False\n","\t l1.encoder.layer.0.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.0.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.0.intermediate.dense.weight False\n","\t l1.encoder.layer.0.intermediate.dense.bias False\n","\t l1.encoder.layer.0.output.dense.weight False\n","\t l1.encoder.layer.0.output.dense.bias False\n","\t l1.encoder.layer.0.output.LayerNorm.weight False\n","\t l1.encoder.layer.0.output.LayerNorm.bias False\n","\t l1.encoder.layer.1.attention.self.query.weight False\n","\t l1.encoder.layer.1.attention.self.query.bias False\n","\t l1.encoder.layer.1.attention.self.key.weight False\n","\t l1.encoder.layer.1.attention.self.key.bias False\n","\t l1.encoder.layer.1.attention.self.value.weight False\n","\t l1.encoder.layer.1.attention.self.value.bias False\n","\t l1.encoder.layer.1.attention.output.dense.weight False\n","\t l1.encoder.layer.1.attention.output.dense.bias False\n","\t l1.encoder.layer.1.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.1.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.1.intermediate.dense.weight False\n","\t l1.encoder.layer.1.intermediate.dense.bias False\n","\t l1.encoder.layer.1.output.dense.weight False\n","\t l1.encoder.layer.1.output.dense.bias False\n","\t l1.encoder.layer.1.output.LayerNorm.weight False\n","\t l1.encoder.layer.1.output.LayerNorm.bias False\n","\t l1.encoder.layer.2.attention.self.query.weight False\n","\t l1.encoder.layer.2.attention.self.query.bias False\n","\t l1.encoder.layer.2.attention.self.key.weight False\n","\t l1.encoder.layer.2.attention.self.key.bias False\n","\t l1.encoder.layer.2.attention.self.value.weight False\n","\t l1.encoder.layer.2.attention.self.value.bias False\n","\t l1.encoder.layer.2.attention.output.dense.weight False\n","\t l1.encoder.layer.2.attention.output.dense.bias False\n","\t l1.encoder.layer.2.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.2.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.2.intermediate.dense.weight False\n","\t l1.encoder.layer.2.intermediate.dense.bias False\n","\t l1.encoder.layer.2.output.dense.weight False\n","\t l1.encoder.layer.2.output.dense.bias False\n","\t l1.encoder.layer.2.output.LayerNorm.weight False\n","\t l1.encoder.layer.2.output.LayerNorm.bias False\n","\t l1.encoder.layer.3.attention.self.query.weight False\n","\t l1.encoder.layer.3.attention.self.query.bias False\n","\t l1.encoder.layer.3.attention.self.key.weight False\n","\t l1.encoder.layer.3.attention.self.key.bias False\n","\t l1.encoder.layer.3.attention.self.value.weight False\n","\t l1.encoder.layer.3.attention.self.value.bias False\n","\t l1.encoder.layer.3.attention.output.dense.weight False\n","\t l1.encoder.layer.3.attention.output.dense.bias False\n","\t l1.encoder.layer.3.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.3.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.3.intermediate.dense.weight False\n","\t l1.encoder.layer.3.intermediate.dense.bias False\n","\t l1.encoder.layer.3.output.dense.weight False\n","\t l1.encoder.layer.3.output.dense.bias False\n","\t l1.encoder.layer.3.output.LayerNorm.weight False\n","\t l1.encoder.layer.3.output.LayerNorm.bias False\n","\t l1.encoder.layer.4.attention.self.query.weight False\n","\t l1.encoder.layer.4.attention.self.query.bias False\n","\t l1.encoder.layer.4.attention.self.key.weight False\n","\t l1.encoder.layer.4.attention.self.key.bias False\n","\t l1.encoder.layer.4.attention.self.value.weight False\n","\t l1.encoder.layer.4.attention.self.value.bias False\n","\t l1.encoder.layer.4.attention.output.dense.weight False\n","\t l1.encoder.layer.4.attention.output.dense.bias False\n","\t l1.encoder.layer.4.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.4.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.4.intermediate.dense.weight False\n","\t l1.encoder.layer.4.intermediate.dense.bias False\n","\t l1.encoder.layer.4.output.dense.weight False\n","\t l1.encoder.layer.4.output.dense.bias False\n","\t l1.encoder.layer.4.output.LayerNorm.weight False\n","\t l1.encoder.layer.4.output.LayerNorm.bias False\n","\t l1.encoder.layer.5.attention.self.query.weight False\n","\t l1.encoder.layer.5.attention.self.query.bias False\n","\t l1.encoder.layer.5.attention.self.key.weight False\n","\t l1.encoder.layer.5.attention.self.key.bias False\n","\t l1.encoder.layer.5.attention.self.value.weight False\n","\t l1.encoder.layer.5.attention.self.value.bias False\n","\t l1.encoder.layer.5.attention.output.dense.weight False\n","\t l1.encoder.layer.5.attention.output.dense.bias False\n","\t l1.encoder.layer.5.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.5.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.5.intermediate.dense.weight False\n","\t l1.encoder.layer.5.intermediate.dense.bias False\n","\t l1.encoder.layer.5.output.dense.weight False\n","\t l1.encoder.layer.5.output.dense.bias False\n","\t l1.encoder.layer.5.output.LayerNorm.weight False\n","\t l1.encoder.layer.5.output.LayerNorm.bias False\n","\t l1.encoder.layer.6.attention.self.query.weight False\n","\t l1.encoder.layer.6.attention.self.query.bias False\n","\t l1.encoder.layer.6.attention.self.key.weight False\n","\t l1.encoder.layer.6.attention.self.key.bias False\n","\t l1.encoder.layer.6.attention.self.value.weight False\n","\t l1.encoder.layer.6.attention.self.value.bias False\n","\t l1.encoder.layer.6.attention.output.dense.weight False\n","\t l1.encoder.layer.6.attention.output.dense.bias False\n","\t l1.encoder.layer.6.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.6.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.6.intermediate.dense.weight False\n","\t l1.encoder.layer.6.intermediate.dense.bias False\n","\t l1.encoder.layer.6.output.dense.weight False\n","\t l1.encoder.layer.6.output.dense.bias False\n","\t l1.encoder.layer.6.output.LayerNorm.weight False\n","\t l1.encoder.layer.6.output.LayerNorm.bias False\n","\t l1.encoder.layer.7.attention.self.query.weight False\n","\t l1.encoder.layer.7.attention.self.query.bias False\n","\t l1.encoder.layer.7.attention.self.key.weight False\n","\t l1.encoder.layer.7.attention.self.key.bias False\n","\t l1.encoder.layer.7.attention.self.value.weight False\n","\t l1.encoder.layer.7.attention.self.value.bias False\n","\t l1.encoder.layer.7.attention.output.dense.weight False\n","\t l1.encoder.layer.7.attention.output.dense.bias False\n","\t l1.encoder.layer.7.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.7.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.7.intermediate.dense.weight False\n","\t l1.encoder.layer.7.intermediate.dense.bias False\n","\t l1.encoder.layer.7.output.dense.weight False\n","\t l1.encoder.layer.7.output.dense.bias False\n","\t l1.encoder.layer.7.output.LayerNorm.weight False\n","\t l1.encoder.layer.7.output.LayerNorm.bias False\n","\t l1.encoder.layer.8.attention.self.query.weight False\n","\t l1.encoder.layer.8.attention.self.query.bias False\n","\t l1.encoder.layer.8.attention.self.key.weight False\n","\t l1.encoder.layer.8.attention.self.key.bias False\n","\t l1.encoder.layer.8.attention.self.value.weight False\n","\t l1.encoder.layer.8.attention.self.value.bias False\n","\t l1.encoder.layer.8.attention.output.dense.weight False\n","\t l1.encoder.layer.8.attention.output.dense.bias False\n","\t l1.encoder.layer.8.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.8.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.8.intermediate.dense.weight False\n","\t l1.encoder.layer.8.intermediate.dense.bias False\n","\t l1.encoder.layer.8.output.dense.weight False\n","\t l1.encoder.layer.8.output.dense.bias False\n","\t l1.encoder.layer.8.output.LayerNorm.weight False\n","\t l1.encoder.layer.8.output.LayerNorm.bias False\n","\t l1.encoder.layer.9.attention.self.query.weight False\n","\t l1.encoder.layer.9.attention.self.query.bias False\n","\t l1.encoder.layer.9.attention.self.key.weight False\n","\t l1.encoder.layer.9.attention.self.key.bias False\n","\t l1.encoder.layer.9.attention.self.value.weight False\n","\t l1.encoder.layer.9.attention.self.value.bias False\n","\t l1.encoder.layer.9.attention.output.dense.weight False\n","\t l1.encoder.layer.9.attention.output.dense.bias False\n","\t l1.encoder.layer.9.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.9.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.9.intermediate.dense.weight False\n","\t l1.encoder.layer.9.intermediate.dense.bias False\n","\t l1.encoder.layer.9.output.dense.weight False\n","\t l1.encoder.layer.9.output.dense.bias False\n","\t l1.encoder.layer.9.output.LayerNorm.weight False\n","\t l1.encoder.layer.9.output.LayerNorm.bias False\n","\t l1.encoder.layer.10.attention.self.query.weight False\n","\t l1.encoder.layer.10.attention.self.query.bias False\n","\t l1.encoder.layer.10.attention.self.key.weight False\n","\t l1.encoder.layer.10.attention.self.key.bias False\n","\t l1.encoder.layer.10.attention.self.value.weight False\n","\t l1.encoder.layer.10.attention.self.value.bias False\n","\t l1.encoder.layer.10.attention.output.dense.weight False\n","\t l1.encoder.layer.10.attention.output.dense.bias False\n","\t l1.encoder.layer.10.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.10.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.10.intermediate.dense.weight False\n","\t l1.encoder.layer.10.intermediate.dense.bias False\n","\t l1.encoder.layer.10.output.dense.weight False\n","\t l1.encoder.layer.10.output.dense.bias False\n","\t l1.encoder.layer.10.output.LayerNorm.weight False\n","\t l1.encoder.layer.10.output.LayerNorm.bias False\n","\t l1.encoder.layer.11.attention.self.query.weight False\n","\t l1.encoder.layer.11.attention.self.query.bias False\n","\t l1.encoder.layer.11.attention.self.key.weight False\n","\t l1.encoder.layer.11.attention.self.key.bias False\n","\t l1.encoder.layer.11.attention.self.value.weight False\n","\t l1.encoder.layer.11.attention.self.value.bias False\n","\t l1.encoder.layer.11.attention.output.dense.weight False\n","\t l1.encoder.layer.11.attention.output.dense.bias False\n","\t l1.encoder.layer.11.attention.output.LayerNorm.weight False\n","\t l1.encoder.layer.11.attention.output.LayerNorm.bias False\n","\t l1.encoder.layer.11.intermediate.dense.weight False\n","\t l1.encoder.layer.11.intermediate.dense.bias False\n","\t l1.encoder.layer.11.output.dense.weight False\n","\t l1.encoder.layer.11.output.dense.bias False\n","\t l1.encoder.layer.11.output.LayerNorm.weight False\n","\t l1.encoder.layer.11.output.LayerNorm.bias False\n","\t l1.pooler.dense.weight False\n","\t l1.pooler.dense.bias False\n","\t lstm.weight_ih_l0 False\n","\t lstm.weight_hh_l0 False\n","\t lstm.bias_ih_l0 False\n","\t lstm.bias_hh_l0 False\n","\t lstm.weight_ih_l0_reverse False\n","\t lstm.weight_hh_l0_reverse False\n","\t lstm.bias_ih_l0_reverse False\n","\t lstm.bias_hh_l0_reverse False\n","\t l3.weight False\n","\t l3.bias False\n"]}],"source":["# print model parametes\n","params_to_update = model.parameters()\n","counter = 0\n","\n","for name, param in model.named_parameters():\n","    if counter > (4): # here we specify layers to unfreeze\n","        param.requires_grad = False\n","        print('\\t', name, param.requires_grad)\n","\n","    else:\n","        param.requires_grad = True\n","        print('\\t', name, param.requires_grad)\n","    counter += 1\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["pos_weights = torch.tensor(pos_weights).to(device)\n","\n","\n","def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)(outputs, targets)\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"elapsed":1052561,"status":"error","timestamp":1650398451158,"user":{"displayName":"Svetlana Maslenkova","userId":"08273567377365638953"},"user_tz":-240},"id":"F-BpuuaZyiFT","outputId":"4f79df61-aef4-40f8-c97b-bfe88379761e"},"outputs":[{"data":{"text/html":["Finishing last run (ID:reez5mkf) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>epoch_train_loss</td><td>▁</td></tr><tr><td>epoch_val_loss</td><td>▁</td></tr><tr><td>global_step</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>▄▄▂▃▆█▃█▁▆▇▃▅▄▄▂▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>epoch_train_loss</td><td>0.95606</td></tr><tr><td>epoch_val_loss</td><td>0.9556</td></tr><tr><td>global_step</td><td>18</td></tr><tr><td>train_loss</td><td>0.96036</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">PRE_ekman_lstm_lr0.00005_emb_unfrozen</strong>: <a href=\"https://wandb.ai/maslenkovas/nlp-project/runs/reez5mkf\" target=\"_blank\">https://wandb.ai/maslenkovas/nlp-project/runs/reez5mkf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220428_115514-reez5mkf/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:reez5mkf). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.15"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/svetlana.maslenkova/Documents/nlp_project/wandb/run-20220428_115636-2dnoi1eb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/maslenkovas/nlp-project/runs/2dnoi1eb\" target=\"_blank\">PRE_FSJ_lstm_lr0.00005_emb_unfrozen</a></strong> to <a href=\"https://wandb.ai/maslenkovas/nlp-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["############# Epoch 1: Training Start   #############\n","############# Epoch 1: Training End     #############\n","############# Epoch 1: Validation Start   #############\n","############# Epoch 1: Validation End     #############\n","Epoch: 1 \tAvgerage Training Loss: 0.956004 \tAverage Validation Loss: 0.955542\n","Validation loss decreased (inf --> 0.955542).  Saving model ...\n","############# Epoch 1  Done   #############\n","\n","############# Epoch 2: Training Start   #############\n","############# Epoch 2: Training End     #############\n","############# Epoch 2: Validation Start   #############\n","############# Epoch 2: Validation End     #############\n","Epoch: 2 \tAvgerage Training Loss: 0.955977 \tAverage Validation Loss: 0.955482\n","Validation loss decreased (0.955542 --> 0.955482).  Saving model ...\n","############# Epoch 2  Done   #############\n","\n","############# Epoch 3: Training Start   #############\n","############# Epoch 3: Training End     #############\n","############# Epoch 3: Validation Start   #############\n","############# Epoch 3: Validation End     #############\n","Epoch: 3 \tAvgerage Training Loss: 0.955944 \tAverage Validation Loss: 0.955422\n","Validation loss decreased (0.955482 --> 0.955422).  Saving model ...\n","############# Epoch 3  Done   #############\n","\n","############# Epoch 4: Training Start   #############\n","############# Epoch 4: Training End     #############\n","############# Epoch 4: Validation Start   #############\n","############# Epoch 4: Validation End     #############\n","Epoch: 4 \tAvgerage Training Loss: 0.955935 \tAverage Validation Loss: 0.955359\n","Validation loss decreased (0.955422 --> 0.955359).  Saving model ...\n","############# Epoch 4  Done   #############\n","\n","############# Epoch 5: Training Start   #############\n","############# Epoch 5: Training End     #############\n","############# Epoch 5: Validation Start   #############\n","############# Epoch 5: Validation End     #############\n","Epoch: 5 \tAvgerage Training Loss: 0.955886 \tAverage Validation Loss: 0.955304\n","Validation loss decreased (0.955359 --> 0.955304).  Saving model ...\n","############# Epoch 5  Done   #############\n","\n","############# Epoch 6: Training Start   #############\n","############# Epoch 6: Training End     #############\n","############# Epoch 6: Validation Start   #############\n","############# Epoch 6: Validation End     #############\n","Epoch: 6 \tAvgerage Training Loss: 0.955874 \tAverage Validation Loss: 0.955259\n","Validation loss decreased (0.955304 --> 0.955259).  Saving model ...\n","############# Epoch 6  Done   #############\n","\n","############# Epoch 7: Training Start   #############\n","############# Epoch 7: Training End     #############\n","############# Epoch 7: Validation Start   #############\n","############# Epoch 7: Validation End     #############\n","Epoch: 7 \tAvgerage Training Loss: 0.955726 \tAverage Validation Loss: 0.955211\n","Validation loss decreased (0.955259 --> 0.955211).  Saving model ...\n","############# Epoch 7  Done   #############\n","\n","############# Epoch 8: Training Start   #############\n","############# Epoch 8: Training End     #############\n","############# Epoch 8: Validation Start   #############\n","############# Epoch 8: Validation End     #############\n","Epoch: 8 \tAvgerage Training Loss: 0.955807 \tAverage Validation Loss: 0.955166\n","Validation loss decreased (0.955211 --> 0.955166).  Saving model ...\n","############# Epoch 8  Done   #############\n","\n","############# Epoch 9: Training Start   #############\n","############# Epoch 9: Training End     #############\n","############# Epoch 9: Validation Start   #############\n","############# Epoch 9: Validation End     #############\n","Epoch: 9 \tAvgerage Training Loss: 0.955784 \tAverage Validation Loss: 0.955116\n","Validation loss decreased (0.955166 --> 0.955116).  Saving model ...\n","############# Epoch 9  Done   #############\n","\n","############# Epoch 10: Training Start   #############\n","############# Epoch 10: Training End     #############\n","############# Epoch 10: Validation Start   #############\n","############# Epoch 10: Validation End     #############\n","Epoch: 10 \tAvgerage Training Loss: 0.955744 \tAverage Validation Loss: 0.955066\n","Validation loss decreased (0.955116 --> 0.955066).  Saving model ...\n","############# Epoch 10  Done   #############\n","\n","############# Epoch 11: Training Start   #############\n","############# Epoch 11: Training End     #############\n","############# Epoch 11: Validation Start   #############\n","############# Epoch 11: Validation End     #############\n","Epoch: 11 \tAvgerage Training Loss: 0.955539 \tAverage Validation Loss: 0.955017\n","Validation loss decreased (0.955066 --> 0.955017).  Saving model ...\n","############# Epoch 11  Done   #############\n","\n","############# Epoch 12: Training Start   #############\n","############# Epoch 12: Training End     #############\n","############# Epoch 12: Validation Start   #############\n","############# Epoch 12: Validation End     #############\n","Epoch: 12 \tAvgerage Training Loss: 0.955602 \tAverage Validation Loss: 0.954967\n","Validation loss decreased (0.955017 --> 0.954967).  Saving model ...\n","############# Epoch 12  Done   #############\n","\n","############# Epoch 13: Training Start   #############\n","############# Epoch 13: Training End     #############\n","############# Epoch 13: Validation Start   #############\n","############# Epoch 13: Validation End     #############\n","Epoch: 13 \tAvgerage Training Loss: 0.955614 \tAverage Validation Loss: 0.954918\n","Validation loss decreased (0.954967 --> 0.954918).  Saving model ...\n","############# Epoch 13  Done   #############\n","\n","############# Epoch 14: Training Start   #############\n","############# Epoch 14: Training End     #############\n","############# Epoch 14: Validation Start   #############\n","############# Epoch 14: Validation End     #############\n","Epoch: 14 \tAvgerage Training Loss: 0.955410 \tAverage Validation Loss: 0.954879\n","Validation loss decreased (0.954918 --> 0.954879).  Saving model ...\n","############# Epoch 14  Done   #############\n","\n","############# Epoch 15: Training Start   #############\n","############# Epoch 15: Training End     #############\n","############# Epoch 15: Validation Start   #############\n","############# Epoch 15: Validation End     #############\n","Epoch: 15 \tAvgerage Training Loss: 0.955618 \tAverage Validation Loss: 0.954839\n","Validation loss decreased (0.954879 --> 0.954839).  Saving model ...\n","############# Epoch 15  Done   #############\n","\n","############# Epoch 16: Training Start   #############\n","############# Epoch 16: Training End     #############\n","############# Epoch 16: Validation Start   #############\n","############# Epoch 16: Validation End     #############\n","Epoch: 16 \tAvgerage Training Loss: 0.955504 \tAverage Validation Loss: 0.954805\n","Validation loss decreased (0.954839 --> 0.954805).  Saving model ...\n","############# Epoch 16  Done   #############\n","\n","############# Epoch 17: Training Start   #############\n","############# Epoch 17: Training End     #############\n","############# Epoch 17: Validation Start   #############\n","############# Epoch 17: Validation End     #############\n","Epoch: 17 \tAvgerage Training Loss: 0.955438 \tAverage Validation Loss: 0.954774\n","Validation loss decreased (0.954805 --> 0.954774).  Saving model ...\n","############# Epoch 17  Done   #############\n","\n","############# Epoch 18: Training Start   #############\n","############# Epoch 18: Training End     #############\n","############# Epoch 18: Validation Start   #############\n","############# Epoch 18: Validation End     #############\n","Epoch: 18 \tAvgerage Training Loss: 0.955361 \tAverage Validation Loss: 0.954746\n","Validation loss decreased (0.954774 --> 0.954746).  Saving model ...\n","############# Epoch 18  Done   #############\n","\n","############# Epoch 19: Training Start   #############\n","############# Epoch 19: Training End     #############\n","############# Epoch 19: Validation Start   #############\n","############# Epoch 19: Validation End     #############\n","Epoch: 19 \tAvgerage Training Loss: 0.955407 \tAverage Validation Loss: 0.954716\n","Validation loss decreased (0.954746 --> 0.954716).  Saving model ...\n","############# Epoch 19  Done   #############\n","\n","############# Epoch 20: Training Start   #############\n","############# Epoch 20: Training End     #############\n","############# Epoch 20: Validation Start   #############\n","############# Epoch 20: Validation End     #############\n","Epoch: 20 \tAvgerage Training Loss: 0.955387 \tAverage Validation Loss: 0.954690\n","Validation loss decreased (0.954716 --> 0.954690).  Saving model ...\n","############# Epoch 20  Done   #############\n","\n","############# Epoch 21: Training Start   #############\n","############# Epoch 21: Training End     #############\n","############# Epoch 21: Validation Start   #############\n","############# Epoch 21: Validation End     #############\n","Epoch: 21 \tAvgerage Training Loss: 0.955329 \tAverage Validation Loss: 0.954667\n","Validation loss decreased (0.954690 --> 0.954667).  Saving model ...\n","############# Epoch 21  Done   #############\n","\n","############# Epoch 22: Training Start   #############\n","############# Epoch 22: Training End     #############\n","############# Epoch 22: Validation Start   #############\n","############# Epoch 22: Validation End     #############\n","Epoch: 22 \tAvgerage Training Loss: 0.955333 \tAverage Validation Loss: 0.954642\n","Validation loss decreased (0.954667 --> 0.954642).  Saving model ...\n","############# Epoch 22  Done   #############\n","\n","############# Epoch 23: Training Start   #############\n","############# Epoch 23: Training End     #############\n","############# Epoch 23: Validation Start   #############\n","############# Epoch 23: Validation End     #############\n","Epoch: 23 \tAvgerage Training Loss: 0.955312 \tAverage Validation Loss: 0.954617\n","Validation loss decreased (0.954642 --> 0.954617).  Saving model ...\n","############# Epoch 23  Done   #############\n","\n","############# Epoch 24: Training Start   #############\n","############# Epoch 24: Training End     #############\n","############# Epoch 24: Validation Start   #############\n","############# Epoch 24: Validation End     #############\n","Epoch: 24 \tAvgerage Training Loss: 0.955434 \tAverage Validation Loss: 0.954593\n","Validation loss decreased (0.954617 --> 0.954593).  Saving model ...\n","############# Epoch 24  Done   #############\n","\n","############# Epoch 25: Training Start   #############\n","############# Epoch 25: Training End     #############\n","############# Epoch 25: Validation Start   #############\n","############# Epoch 25: Validation End     #############\n","Epoch: 25 \tAvgerage Training Loss: 0.955303 \tAverage Validation Loss: 0.954572\n","Validation loss decreased (0.954593 --> 0.954572).  Saving model ...\n","############# Epoch 25  Done   #############\n","\n","############# Epoch 26: Training Start   #############\n","############# Epoch 26: Training End     #############\n","############# Epoch 26: Validation Start   #############\n","############# Epoch 26: Validation End     #############\n","Epoch: 26 \tAvgerage Training Loss: 0.955228 \tAverage Validation Loss: 0.954546\n","Validation loss decreased (0.954572 --> 0.954546).  Saving model ...\n","############# Epoch 26  Done   #############\n","\n","############# Epoch 27: Training Start   #############\n","############# Epoch 27: Training End     #############\n","############# Epoch 27: Validation Start   #############\n","############# Epoch 27: Validation End     #############\n","Epoch: 27 \tAvgerage Training Loss: 0.955258 \tAverage Validation Loss: 0.954520\n","Validation loss decreased (0.954546 --> 0.954520).  Saving model ...\n","############# Epoch 27  Done   #############\n","\n","############# Epoch 28: Training Start   #############\n","############# Epoch 28: Training End     #############\n","############# Epoch 28: Validation Start   #############\n","############# Epoch 28: Validation End     #############\n","Epoch: 28 \tAvgerage Training Loss: 0.955246 \tAverage Validation Loss: 0.954491\n","Validation loss decreased (0.954520 --> 0.954491).  Saving model ...\n","############# Epoch 28  Done   #############\n","\n","############# Epoch 29: Training Start   #############\n","############# Epoch 29: Training End     #############\n","############# Epoch 29: Validation Start   #############\n","############# Epoch 29: Validation End     #############\n","Epoch: 29 \tAvgerage Training Loss: 0.955155 \tAverage Validation Loss: 0.954464\n","Validation loss decreased (0.954491 --> 0.954464).  Saving model ...\n","############# Epoch 29  Done   #############\n","\n","############# Epoch 30: Training Start   #############\n","############# Epoch 30: Training End     #############\n","############# Epoch 30: Validation Start   #############\n","############# Epoch 30: Validation End     #############\n","Epoch: 30 \tAvgerage Training Loss: 0.955221 \tAverage Validation Loss: 0.954436\n","Validation loss decreased (0.954464 --> 0.954436).  Saving model ...\n","############# Epoch 30  Done   #############\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epoch_train_loss</td><td>███▇▇▇▆▆▆▆▄▅▅▃▅▄▃▃▃▃▂▂▂▃▂▂▂▂▁▂</td></tr><tr><td>epoch_val_loss</td><td>██▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>global_step</td><td>▄▇▆▅▁█▇▃▂▁▅▄▃▂▅▅▄▇▆▅▁█▇▃▂▁█▄▃▂▅▅▄▇▆▅▁█▇▆</td></tr><tr><td>train_loss</td><td>▄▂▄▇▅▇▅▆█▆▂▃▂▅▅▇▅▄▂▄▅▆██▆▁▆▇▅▂▆▆▄▄▆▄▁▄▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>epoch_train_loss</td><td>0.95522</td></tr><tr><td>epoch_val_loss</td><td>0.95444</td></tr><tr><td>global_step</td><td>18</td></tr><tr><td>train_loss</td><td>0.96228</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">PRE_FSJ_lstm_lr0.00005_emb_unfrozen</strong>: <a href=\"https://wandb.ai/maslenkovas/nlp-project/runs/2dnoi1eb\" target=\"_blank\">https://wandb.ai/maslenkovas/nlp-project/runs/2dnoi1eb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220428_115636-2dnoi1eb/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["checkpoint_path = cwd+'/checkpoints/PRE_FSJ_lstm_lr0.00005_emb_unfreeze.pt'\n","\n","\n","wandb.init(project='nlp-project', name='PRE_FSJ_lstm_lr0.00005_emb_unfreeze')\n","trained_model = train_model(1, 30, np.Inf, train_loader, valid_loader, model, \\\n","                      optimizer, checkpoint_path) \n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["### EVALUATION"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model with LSTM layer\n","class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n","        self.l2 = torch.nn.Dropout(0.3)\n","        self.lstm = nn.LSTM(input_size=768,\n","                            hidden_size=128,\n","                            num_layers=1,\n","                            batch_first=True,\n","                            bidirectional=True)\n","        self.l3 = torch.nn.Linear(256, 6)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        _, output= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n","        output = self.l2(output).unsqueeze(1)\n","        output, _ = self.lstm(output)\n","        output = self.l3(output).squeeze(1)\n","        return output\n","        \n","## model without LSTM layer\n","# class BERTClass(torch.nn.Module):\n","#     def __init__(self):\n","#         super(BERTClass, self).__init__()\n","#         self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n","#         self.l2 = torch.nn.Dropout(0.3)\n","#         self.l3 = torch.nn.Linear(768, 6)\n","    \n","#     def forward(self, ids, mask, token_type_ids):\n","#         _, output= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n","#         output = self.l2(output)\n","#         output = self.l3(output)\n","#         return output\n","\n","model = BERTClass()"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[],"source":["# checkpoint_path = '/home/svetlanamaslenkova/Documents/nlp_project/checkpoints/pretrained_model_full_ekman_lstm.pt'\n","\n","def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)(outputs, targets)\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w1AYa8M5qCpr"},"outputs":[],"source":["load_ckp(checkpoint_path, model=model, optimizer=optimizer, gpu=False)"]},{"cell_type":"code","execution_count":100,"metadata":{"id":"WrInmp2b2KBp"},"outputs":[],"source":["device = 'cpu'\n","model = model.to(device)"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"43YnGwnjZZOH"},"outputs":[],"source":["sigmoid = torch.nn.Sigmoid()\n","targets_list = []\n","preds_list = []\n","\n","for batch_idx, data in enumerate(test_loader):\n","    #print('yyy epoch', batch_idx)\n","    # ids = data['ids'].to(device, dtype = torch.long)\n","    # mask = data['mask'].to(device, dtype = torch.long)\n","    # token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","    # targets = data['targets'].to(device, dtype = torch.float)\n","    ids = data['ids'].to(device)\n","    mask = data['mask'].to(device)\n","    token_type_ids = data['token_type_ids'].to(device)\n","    targets = data['targets'].to(device)\n","\n","# outputs  = model_ckp(ids, mask, token_type_ids)\n","\n","    outputs = sigmoid(model(ids, mask, token_type_ids))\n","    val_preds = (np.array(outputs.cpu().detach().numpy()) > 0.5).astype(int)\n","    targets = np.array(targets)\n","    # print(targets.shape)\n","    # print(val_preds.shape)\n","\n","    if batch_idx == 0:\n","        targets_list = targets\n","        preds_list = val_preds\n","    else:\n","        targets_list = np.concatenate((targets_list, targets))\n","        preds_list = np.concatenate((preds_list, val_preds))"]},{"cell_type":"code","execution_count":104,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1650389164377,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"9MCpKbeRlq05","outputId":"84f5abd7-af66-4fa1-bb34-208fe1d0597b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy Score = 0.8\n","F1 Score (Micro) = 0.82\n","F1 Score (Macro) = 0.79\n"]}],"source":["from sklearn import metrics\n","\n","accuracy = np.round(metrics.accuracy_score(targets_list, preds_list), 2)\n","f1_score_micro = np.round(metrics.f1_score(targets_list, preds_list, average='micro'), 2)\n","f1_score_macro = np.round(metrics.f1_score(targets_list, preds_list, average='macro'), 2)\n","print(f\"Accuracy Score = {accuracy}\")\n","print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","print(f\"F1 Score (Macro) = {f1_score_macro}\")"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.73      0.70       146\n","           1       0.85      0.76      0.80       342\n","           2       0.85      0.88      0.87       410\n","\n","   micro avg       0.82      0.81      0.82       898\n","   macro avg       0.79      0.79      0.79       898\n","weighted avg       0.82      0.81      0.82       898\n"," samples avg       0.82      0.81      0.82       898\n","\n"]},{"name":"stderr","output_type":"stream","text":["/home/svetlana.maslenkova/.conda/envs/nlp_project_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["print(classification_report(targets_list, preds_list, output_dict=False))"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1650389204280,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"a9-Yof1UmREM","outputId":"e5534a31-fb43-4982-975f-fb531b3f46ae"},"outputs":[{"data":{"text/plain":["array([[[688,  50],\n","        [ 40, 106]],\n","\n","       [[496,  46],\n","        [ 81, 261]],\n","\n","       [[412,  62],\n","        [ 50, 360]]])"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["cf = metrics.multilabel_confusion_matrix(targets_list, preds_list)\n"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"dxmnGrl8ang-"},"outputs":[],"source":["y_true = {}\n","y_pred = {}\n","if ekman_emotions:\n","    y_true['joy'], y_pred['joy'] = targets_list[:, 0], preds_list[:,0]\n","    y_true['fear'], y_pred['fear']= targets_list[:, 1], preds_list[:,1]\n","    y_true['anger'], y_pred['anger'] = targets_list[:, 2], preds_list[:,2]\n","    y_true['surprise'], y_pred['surprise'] = targets_list[:, 3], preds_list[:,3]\n","    y_true['disgust'], y_pred['disgust'] = targets_list[:, 4], preds_list[:,4]\n","    y_true['sadness'], y_pred['sadness'] = targets_list[:, 5], preds_list[:,5]\n","if sentiment:\n","    y_true['positive'], y_pred['positive'] = targets_list[:, 0], preds_list[:,0]\n","    y_true['negative'], y_pred['negative']= targets_list[:, 1], preds_list[:,1]\n","    y_true['neutral'], y_pred['neutral'] = targets_list[:, 2], preds_list[:,2]\n","    y_true['mixed'], y_pred['mixed'] = targets_list[:, 3], preds_list[:,3]\n","\n","if full_ekman_emotions:\n","    y_true['anger'], y_pred['anger'] = targets_list[:, 0], preds_list[:,0]\n","    y_true['disguist'], y_pred['disguist']= targets_list[:, 1], preds_list[:,1]\n","    y_true['joy'], y_pred['joy'] = targets_list[:, 2], preds_list[:,2]\n","    y_true['sadness'], y_pred['sadness'] = targets_list[:, 3], preds_list[:,3]\n","    y_true['surprise'], y_pred['surprise'] = targets_list[:, 4], preds_list[:,4]\n","    y_true['neutral'], y_pred['neutral'] = targets_list[:, 5], preds_list[:,5]\n","\n","if sentiment_3:\n","    y_true['positive'], y_pred['positive'] = targets_list[:, 0], preds_list[:,0]\n","    y_true['negative'], y_pred['negative']= targets_list[:, 1], preds_list[:,1]\n","    y_true['ambiguous'], y_pred['ambiguous'] = targets_list[:, 2], preds_list[:,2]\n","\n","if FSJ_emotions:\n","    y_true['fear'], y_pred['fear'] = targets_list[:, 0], preds_list[:,0]\n","    y_true['sadness'], y_pred['sadness']= targets_list[:, 1], preds_list[:,1]\n","    y_true['joy'], y_pred['joy'] = targets_list[:, 2], preds_list[:,2]"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1650389216703,"user":{"displayName":"Svetlana Maslenkova","userId":"16443472050162924369"},"user_tz":-240},"id":"Y79J-Ct5gHXp","outputId":"419ef308-ee15-4cf4-88b8-42dc6124e228"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1-score for fear: 0.7\n","F1-score for sadness: 0.8\n","F1-score for joy: 0.87\n"]}],"source":["f1 = {}\n","if ekman_emotions:\n","    emotions = ['joy',\t'fear',\t'anger',\t'surprise',\t'disgust',\t'sadness']\n","if sentiment:\n","    emotions = ['positive',\t'negative',\t'neutral',\t'mixed']\n","if full_ekman_emotions:\n","    emotions = ['anger',\t'disguist',\t'joy',\t'sadness',\t'surprise',\t'neutral']\n","if sentiment_3:\n","    emotions = ['positive',\t'negative',\t'ambiguous']\n","if FSJ_emotions:\n","    emotions = ['fear', 'sadness', 'joy']\n","\n","\n","for emotion in emotions:\n","    f1[emotion] = np.round(metrics.f1_score(y_true[emotion], y_pred[emotion]), 2)\n","    print(f'F1-score for {emotion}: {f1[emotion]}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"bert_emotions_ft.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
